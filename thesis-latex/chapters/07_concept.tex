\chapter{Concept}\label{chap:concept}
As we have seen in the previous chapters \ref{chap:risc-v} and
\ref{chap:mod_risc-v} the Rocket Chip SoC generator is build
to be extensible and adaptable.
This modularity allows for new concepts when it comes to implementing
Checkpoint/Restore mechanisms.
The following sections will present one
way of implementing a Checkpoint/Restore in
a customizable architecture like RISC-V.

\section{Virtual sliced Checkpoint/Restore}
A big problem of previous implementations of
Checkpoint/Restore mechanism is often the tracking of checkpointed
pages (or any other size one chooses to checkpoint) and the link
to the working copy that should be used for write operations
which occur after the checkpoint was put in place.
The operations take computing time and memory.

\subsection{Extending the page table entries and the memory management unit}
We want to start by adding two Boolean entries
to the already existing page table entries (PTEs).
\begin{itemize}
    \item Checkpoint active (CA)
    \item Checkpoint written (CW)
\end{itemize}
When the CA flag is active write operations for this memory
page will be redirected to the working copy of the page.
If the CW flag is not set a copy operation will be performed
before the write operation to copy the complete data set to
the location of the working copy.
A read operation will only be redirected when the CW flag
is set to true.
This system does work for a single checkpoint but also works
for multiple checkpoints, as the redirected PTE on its own contains
the two flags which again can trigger a redirect to a new PTE.
A check should be included to avoid endless loops.
If the prefix overflows from 1...1 to the original base
prefix 0...0 an error should be raised.

\subsection{Addressing the working copy}
To make addressing a working copy easier and less complicated
we use the large virtual address space of a 64-bit processor
and split it up into \textit{n} by power of two pieces of the same size.
The value of \textit{n} defines the maximum count of possible checkpoints
(including the current working copy) the system can hold.
Thanks to the splitting of the virtual address space we now
can address working copies with a simple prefix.
The system will at startup only be presented with the first
block with the prefix 0...0 and is unable to directly
write to other blocks.
To illustrate the process better, let us assume we pick $n=2$.
A system will, in the normal operation,
work on the virtual memory part with the prefix 0. Let us assume
further that our system sets the checkpointing flag for the virtual
page 00...4242 ("..." short for 0s) and wants to execute
a write operation on this page.
As the CA flag on the PTE for 00...4242 is set, but the CW flag
is not, the MMU will trigger a trap, 
called Copy-on-Write (CoW) interruption.
This trap will cause the content of page 00...4242
will be copied to the page 10...4242. After this operation, 
every write access to the PTE 00...4242 will be redirected to
the PTE 10...4242 as long as the CA flag is set in PTE.
This system can easily be applied to $n=2^i,i \in \mathbb{N}_{\ne 0}$
as the redirects will recursively lead to the current working
copy PTE.

\subsection{Restore process}
To restore the previously checkpointed state of the pages,
the system should block the access to the address ranges.
This lock will prevent the possibility of corrupt data during
the restore process. A full halt of the program execution
might not be needed. A waitForCheckpoint trap, 
which is thrown when a certain memory page is accessed,
should be sufficient. This trap then waits for the
CA flag to be false. To increase the performance,
it might be useful that the trap can also signal a
request for prioritization to the restore process.

\subsection{Deletion process}
A deletion process can be easily implemented, if the
processor supports locking PTEs
(RISC-V T extension, \ref{chap:transactional_memory_instructions}).
Then the original PTE can be locked, while the
data of the working copy PTE is copied back.
If the CPU does not provide the support for
locking a page, we need to implement a similar
lock like in the previous subsection.

\subsection{Expected performance and limitations}
\subsubsection{Virtual address space}
The first obvious limitation is the splitting of the addressable
virtual memory space. If or if not this impacts the execution
of a system depends on the work task and the available
virtual memory space the processor does provide.
32-bit implementations might often be ruled out, as splitting
the already limited space of 32-bit into smaller pieces
limits the applications the processor can be used for. As for 64-bit applications,
this problem is far less serious, as long as the
number of needed checkpointing slots is limited.
The RISC-V 64-bit implementation defines up to 48 bits of addressable
memory space \cite[p.~63]{risc-v_isa_manual_priviliged_arch},
which translates to $2^{48} = 281,474,976,710,656$.
For a system with a predefined value of $n=4$ still
$2^{48}/4 = 2^{46} = 70,368,744,177,664$ addresses will be available
and four system states can be maintained.
This equals roughly 70TB of addressable memory.
If this limit is reached, one could reduce the number
of available checkpoints or switch to a 128-bit implementation
of a processor (like the RISC-V 128-bit base)
which provides huge addressable memory spaces.

\subsubsection{Additional memory usage}
As we add two boolean values to the PTEs, we decrease the
number of elements the translation lookaside buffer (TLB) of the
MMU can hold. Even if the PTE is only 64 bit in size, we
increase the size to only 66, a $66/44 - 1 = 3,125\%$
of additional memory.
A bigger problem could be the trashing of the TLB.
When checkpoints are created future read and write
operations need to find the new working copy that
should be used recursively. This generates the problem that
for a single memory page \textit{n} PTEs are necessary to
get to the actual working copy.
To negate this problem a processor might increase the size
of the TLB, which can increase the performance
of the system not only in checkpointing use cases
but also in the normal operation.

\subsubsection{Expected performance}
The performance of PTE checkpointed memory page operations
solely relies on the implementation of the MMU.
If the MMU can check the checkpointed recursive PTE paths
fast enough, there will be only an insignificant loss
of performance due to the increased size of the PTEs.
